{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"logo_neuralearn.png\" height=\"100\" width= \"100\" >\n",
    " <h1> <center> <font color = \"black\"> [Deep Learning with TensorFlow 2]   <br><font color=\"grey\"> PARKINSON DISEASE DETECTION </font></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, LayerNormalization, Conv2DTranspose, Dropout, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from IPython.display import Image, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> DATA PREPARATION </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, data, batch_size, IN_1, IN_2, shuffle = False):\n",
    "        \n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.data_list = os.listdir(data)\n",
    "        self.IN_1 = IN_1\n",
    "        self.IN_2 = IN_2\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.data_list)/self.batch_size) )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X, y = self.__data_generation(idx)\n",
    "        \n",
    "        return X,y\n",
    "    \n",
    "    def get_spec(self, inp):\n",
    "    \n",
    "        zero_padding = (12000 - len(inp))*[0]\n",
    "        zero_padding = tf.constant(zero_padding, tf.float32)\n",
    "        inp = tf.cast(inp, tf.float32)\n",
    "        equal_length = tf.concat([inp, zero_padding], 0)\n",
    "\n",
    "        spectrogram = tf.signal.stft(\n",
    "          equal_length, frame_length = 256, frame_step = 127)\n",
    "\n",
    "        spectrogram = tf.math.log(tf.abs(spectrogram))\n",
    "\n",
    "        return np.resize(tf.expand_dims(spectrogram, axis = -1), (self.IN_1,self.IN_2,1))\n",
    "\n",
    "\n",
    "    def __data_generation(self, idx):\n",
    "        \n",
    "        X_1, X_2, X_4 = [],[],[]\n",
    "        y = []\n",
    "        \n",
    "        for j in range(idx*self.batch_size, (idx+1)*self.batch_size ):\n",
    "\n",
    "            data_list = []\n",
    "            f = open(self.data + self.data_list[idx], \"r\")\n",
    "            for x in f:\n",
    "                data_list.append([int(x) for x in x[:-1].split(';')])\n",
    "            data_list = np.array(data_list, dtype = float)\n",
    "            \n",
    "            data_list[:,0] = (data_list[:,0] - np.mean(data_list[:,0]))/np.std(data_list[:,0])\n",
    "            data_list[:,1] = (data_list[:,1] - np.mean(data_list[:,1]))/np.std(data_list[:,1])\n",
    "            data_list[:,3] = (data_list[:,3] - np.mean(data_list[:,3]))/np.std(data_list[:,3])\n",
    "            \n",
    "            X_1.append(self.get_spec(tf.constant(data_list)[:,0]))\n",
    "            X_2.append(self.get_spec(tf.constant(data_list)[:,1]))\n",
    "            #X_3.append(self.get_spec(tf.constant(data_list)[:,2]))\n",
    "            X_4.append(self.get_spec(tf.constant(data_list)[:,3]))\n",
    "            #X_5.append(self.get_spec(tf.constant(data_list)[:,4]))\n",
    "            #X_6.append(self.get_spec(tf.constant(data_list)[:,5]))\n",
    "            #X_7.append(self.get_spec(tf.constant(data_list)[:,6]))\n",
    "            \n",
    "            if(self.data_list[idx][0] == 'C'):\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)\n",
    "            \n",
    "            \n",
    "        return [tf.convert_to_tensor(X_1), tf.convert_to_tensor(X_2),tf.convert_to_tensor(X_4)], tf.convert_to_tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = 'D:/Machine_Learning/datasets/PARKINSON_HW/parkinson/OTHER/'\n",
    "IN_1, IN_2 = 64, 64\n",
    "BATCH_SIZE = 2\n",
    "EPOCH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(train_images, BATCH_SIZE, IN_1, IN_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MODELING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayers(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, NUM_FILTERS):\n",
    "        super(ConvLayers, self).__init__()\n",
    "        self.conv = Conv2D(NUM_FILTERS, 5, padding = 'same', activation ='relu',)\n",
    "        self.pool = MaxPooling2D(strides = 2)\n",
    "        \n",
    "    def call(self, x, ):\n",
    "        \n",
    "        x = self.pool(self.conv(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [tf.keras.Input(shape=(IN_1, IN_2, 1)) for _ in range(3)]\n",
    "\n",
    "x_out = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    \n",
    "    #x = tf.keras.layers.experimental.preprocessing.Normalization()(inputs[i])\n",
    "    \n",
    "    x = ConvLayers(32)(inputs[i])\n",
    "    x = ConvLayers(64)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x_out.append(x)\n",
    "    \n",
    "\n",
    "x_out = tf.concat(x_out, axis = -1)\n",
    "\n",
    "x_out = Dense(16, activation = 'relu')(x_out)\n",
    "x_out = Dense(1, activation = 'sigmoid')(x_out)\n",
    "\n",
    "model = tf.keras.Model(inputs = inputs, outputs = x_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> TRAINING AND OPTIMIZATION </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = Adam(lr = LR,),\n",
    "    metrics = 'accuracy',\n",
    "    #run_eagerly = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'parkinson/cnn_dense.hdf5'\n",
    "\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "# Train model on dataset\n",
    "history = model.fit(train_gen, shuffle = True, epochs = EPOCH, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> TESTING </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = 'D:/Machine_Learning/datasets/PARKINSON_HW/parkinson/OTHER/C_0012.txt'#C_0013\n",
    "\n",
    "X_1, X_2, X_4 = [], [], []\n",
    "\n",
    "data_list = []\n",
    "\n",
    "def get_spec(inp):\n",
    "\n",
    "    zero_padding = (12000 - len(inp))*[0]\n",
    "    zero_padding = tf.constant(zero_padding, tf.float32)\n",
    "    inp = tf.cast(inp, tf.float32)\n",
    "    equal_length = tf.concat([inp, zero_padding], 0)\n",
    "\n",
    "    spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length = 256, frame_step = 127)\n",
    "\n",
    "    spectrogram = tf.math.log(tf.abs(spectrogram))\n",
    "\n",
    "    return np.resize(tf.expand_dims(spectrogram, axis = -1), (IN_1,IN_2,1))\n",
    "\n",
    "    \n",
    "f = open(test_image, \"r\")\n",
    "for x in f:\n",
    "    data_list.append([int(x) for x in x[:-1].split(';')])\n",
    "data_list = np.array(data_list, dtype = float)\n",
    "            \n",
    "data_list[:,0] = (data_list[:,0] - np.mean(data_list[:,0]))/np.std(data_list[:,0])\n",
    "data_list[:,1] = (data_list[:,1] - np.mean(data_list[:,1]))/np.std(data_list[:,1])\n",
    "data_list[:,3] = (data_list[:,3] - np.mean(data_list[:,3]))/np.std(data_list[:,3])\n",
    "            \n",
    "X_1.append(get_spec(tf.constant(data_list)[:,0]))\n",
    "X_2.append(get_spec(tf.constant(data_list)[:,1]))\n",
    "X_4.append(get_spec(tf.constant(data_list)[:,3]))\n",
    "\n",
    "X = [tf.constant(X_1), tf.constant(X_2), tf.constant(X_4)]\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
